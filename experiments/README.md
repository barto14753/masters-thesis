# Experiments

## Run ollama server with open-webui chat GUI

```bash
docker-compose up
```

## Install models

Download models using ollama inside container

```bash
# Example pull
ollama pull llama3
```

### Models

- llama3
- mistral

## OpenWebUI

Available in browser at `http://localhost:8080`

## Ollama server

Available at `http://localhost:7869`
