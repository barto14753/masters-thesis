# `Analysis of attacks on language models`

- _Author_: `Bartosz Włodarski`
- _Supervisor_:
- _University_: `Cracow University of Technology`
- _Faculty_: `Faculty of Computer Science and Telecommunications`
- _Field of study_: `Computer Science`
- _Specialization_: `Cybersecurity`
- _Degree_: `Master of Engineering`
- _Language_: `English`
- _Year_: `2024-2025`

# Introduction to Language Models

## 1. Definition of a Language Model

## 2. Capabilities of a Language Model

## 3. Why Language Models? Impact on Technology, Economy, and Society

## 4. History of Language Models

### 4.1. Rule-based Natural Language Processing

### 4.2. Statistical Models

### 4.3. Neural Network-based Models

### 4.4. Transformer-based Models

### 4.5. Multimodal Models

## 5. Deployment of Language Models

## 6. Challenges

### 6.1. Computational Power Requirements

### 6.2. Training Costs

### 6.3. Errors and Hallucinations

## 7. Threats and Risks of Using Language Models

# Objectives of the Study

## 1. Discussion of the “OWASP Top 10 for LLM Applications 2025” list

## 2. Description of the environment used for testing attacks

## 3. Presentation of attacks on locally deployed language models

## 4. Presentation of defense techniques against attacks

# OWASP Top 10 for LLM Applications 2025

## 1. What is the list and what are its goals?

## 2. Previous editions of the list

## 3. The Top 10 List:

### 3.1. LLM01: Prompt Injection

### 3.2. LLM02: Sensitive Information Disclosure

### 3.3. LLM03: Supply Chain

### 3.4. LLM04: Data and Model Poisoning

### 3.5. LLM05: Improper Output Handling

### 3.6. LLM06: Excessive Agency

### 3.7. LLM07: System Prompt Leakage

### 3.8. LLM08: Vector and Embedding Weaknesses

### 3.9. LLM09: Misinformation

### 3.10. LLM10: Unbounded Consumption

## 4. Summary: LLM Application Architecture with Possible Attack Vectors

# Testing Environment

## 1. Ollama

### 1.1. Models repository

### 1.2. Python SDK

## 2. OpenWebUI

## 3. Local Models

### 3.1. LLama3

### 3.2. Mistral

### 3.3. Custom Models

### 3.4. Why self-hosted models?

## 4. Hardware

# Attacks on Language Models

## 1. Basic Prompt Injection

## 2. Prompt Leak

## 3. Jailbreak via Role-Playing

## 4. Reverse Prompt Injection

## 5. Token Smuggling

## 6. Unicode Trickery

## 7. Semantic Redirection

## 8. Context Misdirection

## 9. Multi-Prompt Simulation

## 10. Code Completion Attack

# Defense Techniques Against Attacks on Language Models

## 1. Sanitized Prompting

### 1.1. Simple Example

### 1.2. Secure Finance Assistant

## 2. Output Filtering

### 2.1. Simple Filter

### 2.2. Data Sanitization Agent

## 3. Rate Limiting

## 4. Intent Detection

## 5. Watermarking

# Summary

## 1. Main Attack Vectors

## 2. Effectiveness of Defense Mechanisms

## 3. Challenges in Securing Language Models

## 4. Future Directions and Research Areas
